{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import *\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read all the csv files\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('input/*.csv')}\n",
    "for k, v in dfs.items(): locals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The holidays happened on weekends have no significant difference in visit\n",
    "wkend_holidays = date_info.apply(lambda x: (x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1, axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "# Earlier record has little weight, because the reservation population are generally smaller than later\n",
    "# We want to take a basis of the visit number\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge visit_data and date_info on the visit date\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "# Process by log(1+x) for visit number\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We get a visitors df to store the weighted visitor number for each group of store_id, day_of_week and holiday_flg\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )).reset_index()\n",
    "# 829 unique store id\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a basis visit number for similar record\n",
    "# Parse the store_id and date from the column id\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "# Drop the default column\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "# To fill the other group columns, dow and holiday\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "# Merge them to get the basis\n",
    "# 821 unique store id, all covered in visitors\n",
    "sample_submission = sample_submission.merge(visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'air_0ead98dd07e7a82a',\n",
       " 'air_229d7e508d9f1b5e',\n",
       " 'air_2703dcb33192b181',\n",
       " 'air_b2d8bc9c88b85f96',\n",
       " 'air_cb083b4789a8d3a2',\n",
       " 'air_cf22e368c1a71d53',\n",
       " 'air_d0a7bd3339c3d12a',\n",
       " 'air_d63cfa6d6ab78446'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(visitors.air_store_id.unique()) - set(sample_submission.air_store_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If there are missing value(actually a lot), merge them with same weekday and store_id, only take none-holiday, otherwise\n",
    "# there will be duplicates\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If still have null value, merge with the mean visit number of all records\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), on='air_store_id', how='left')['visitors_y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e^y - 1\n",
    "test_visit_var = sample_submission.visitors.map(pd.np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge target visit data(label) with date_info\n",
    "data = {\n",
    "    'tra': pd.read_csv('input/air_visit_data.csv')\n",
    "    }\n",
    "\n",
    "data['tra'] = data['tra'].merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "data['tra'] = data['tra'].merge(visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actually, there would be no missing value here\n",
    "missings = data['tra'].visitors_y.isnull()\n",
    "data['tra'].loc[missings, 'visitors_y'] = data['tra'][missings].merge(visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n",
    "\n",
    "missings = data['tra'].visitors_y.isnull()\n",
    "data['tra'].loc[missings, 'visitors_y'] = data['tra'][missings].merge(visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "train_visit_var = data['tra'].visitors_y.map(pd.np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('input/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('input/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('input/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('input/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('input/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('input/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('input/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the reservation in test data\n",
    "\n",
    "1. The last reservation time is 2017-04-22 23:00 while the last visit time is 2017-05-31\n",
    "2. This model just merge test data with the reservation features, of which the lag is far larger than the part in training data\n",
    "3. We only consider reservation taken 40 days before the visit date to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge only on the hotels that both have hpg and air id, so the number of data decreases a lot\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime']).dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    # 1: sum  2: mean of diff and visitor number\n",
    "    # Only take 40 days diff\n",
    "    data[df] = data[df][data[df]['reserve_datetime_diff'] >= 40]\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract day features from visit_date column\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract day features from sample submission\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by store and dow, get the min, mean ... of each group\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "#OPTIMIZED BY JEROME VALLET\n",
    "tmp = data['tra'].groupby(['air_store_id','dow']).agg({'visitors' : [np.min,np.mean,np.median,np.max,np.size]}).reset_index()\n",
    "tmp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>dow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>min_visitors</th>\n",
       "      <th>mean_visitors</th>\n",
       "      <th>...</th>\n",
       "      <th>air_genre_name5</th>\n",
       "      <th>air_area_name5</th>\n",
       "      <th>air_genre_name6</th>\n",
       "      <th>air_area_name6</th>\n",
       "      <th>air_genre_name7</th>\n",
       "      <th>air_area_name7</th>\n",
       "      <th>air_genre_name8</th>\n",
       "      <th>air_area_name8</th>\n",
       "      <th>air_genre_name9</th>\n",
       "      <th>air_area_name9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.843750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.292308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.651515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.754386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  visitors  dow  year  month  day_of_week  \\\n",
       "0  air_ba937bf13d40fb24  2016-01-13        25    2  2016      1            6   \n",
       "1  air_ba937bf13d40fb24  2016-01-14        32    3  2016      1            4   \n",
       "2  air_ba937bf13d40fb24  2016-01-15        29    4  2016      1            0   \n",
       "3  air_ba937bf13d40fb24  2016-01-16        22    5  2016      1            2   \n",
       "4  air_ba937bf13d40fb24  2016-01-18         6    0  2016      1            1   \n",
       "\n",
       "   holiday_flg  min_visitors  mean_visitors       ...        air_genre_name5  \\\n",
       "0            0           7.0      23.843750       ...                    0.0   \n",
       "1            0           2.0      20.292308       ...                    0.0   \n",
       "2            0           4.0      34.738462       ...                    0.0   \n",
       "3            0           6.0      27.651515       ...                    0.0   \n",
       "4            0           2.0      13.754386       ...                    0.0   \n",
       "\n",
       "   air_area_name5  air_genre_name6  air_area_name6  air_genre_name7  \\\n",
       "0             0.0              0.0             0.0              0.0   \n",
       "1             0.0              0.0             0.0              0.0   \n",
       "2             0.0              0.0             0.0              0.0   \n",
       "3             0.0              0.0             0.0              0.0   \n",
       "4             0.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name7  air_genre_name8  air_area_name8  air_genre_name9  \\\n",
       "0             0.0              0.0             0.0              0.0   \n",
       "1             0.0              0.0             0.0              0.0   \n",
       "2             0.0              0.0             0.0              0.0   \n",
       "3             0.0              0.0             0.0              0.0   \n",
       "4             0.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name9  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new year and golden week feature\n",
    "newyear_dates = pd.date_range(datetime(2016,1,1),datetime(2016,1,5)).append(pd.date_range(datetime(2016,12,31),datetime(2017,1,5)))\n",
    "goldweek = pd.date_range(datetime(2016,4,29),datetime(2016,5,5)).append(pd.date_range(datetime(2017,4,29),datetime(2017,5,5)))\n",
    "train['visit_date'] = pd.to_datetime(train['visit_date'].values)\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'].values)\n",
    "train['newyear'] = train['visit_date'].isin(newyear_dates)\n",
    "train['goldweek'] = train['visit_date'].isin(goldweek)\n",
    "test['newyear'] = test['visit_date'].isin(newyear_dates)\n",
    "test['goldweek'] = test['visit_date'].isin(goldweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge true visit, holiday, store with air_reserve and hpg_reserve\n",
    "# Big problem in this step\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After merge air and hpg, we have rv and rs from two systems\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM JMBULL\n",
    "# Interesting feature, take date as feature, because later time, the number of visitors is larger\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "train['hklee_feature'] = train_visit_var \n",
    "test['hklee_feature'] = test_visit_var\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'air_area_name': 6949,\n",
       " 'air_area_name0': 533,\n",
       " 'air_area_name1': 615,\n",
       " 'air_area_name2': 2405,\n",
       " 'air_area_name3': 352,\n",
       " 'air_area_name4': 3717,\n",
       " 'air_area_name5': 497,\n",
       " 'air_area_name6': 144,\n",
       " 'air_genre_name': 3573,\n",
       " 'air_genre_name0': 445,\n",
       " 'air_genre_name1': 1914,\n",
       " 'air_genre_name2': 114,\n",
       " 'air_store_id2': 7420,\n",
       " 'avg_humidity': 2027,\n",
       " 'avg_local_pressure': 3564,\n",
       " 'avg_sea_pressure': 1031,\n",
       " 'avg_temperature': 3476,\n",
       " 'avg_vapor_pressure': 2065,\n",
       " 'avg_wind_speed': 3550,\n",
       " 'cloud_cover': 1663,\n",
       " 'count_observations': 6771,\n",
       " 'date_int': 8281,\n",
       " 'day_of_week': 1404,\n",
       " 'dow': 1160,\n",
       " 'goldweek': 107,\n",
       " 'high_temperature': 2883,\n",
       " 'hklee_feature': 10409,\n",
       " 'holiday_flg': 325,\n",
       " 'hours_sunlight': 2847,\n",
       " 'latitude': 2252,\n",
       " 'lon_plus_lat': 1290,\n",
       " 'longitude': 1073,\n",
       " 'low_temperature': 2617,\n",
       " 'max_visitors': 7206,\n",
       " 'mean_visitors': 7773,\n",
       " 'median_visitors': 3657,\n",
       " 'min_visitors': 3008,\n",
       " 'month': 1452,\n",
       " 'month0': 40,\n",
       " 'month1': 188,\n",
       " 'month10': 174,\n",
       " 'month11': 61,\n",
       " 'month2': 225,\n",
       " 'month3': 140,\n",
       " 'month4': 128,\n",
       " 'month5': 161,\n",
       " 'month6': 153,\n",
       " 'month7': 158,\n",
       " 'month8': 176,\n",
       " 'month9': 187,\n",
       " 'newyear': 132,\n",
       " 'precipitation': 1824,\n",
       " 'solar_radiation': 3018,\n",
       " 'var_max_lat': 591,\n",
       " 'var_max_long': 432,\n",
       " 'weekday0': 38,\n",
       " 'weekday1': 202,\n",
       " 'weekday2': 41,\n",
       " 'weekday3': 176,\n",
       " 'weekday4': 48,\n",
       " 'weekday5': 168,\n",
       " 'weekday6': 36,\n",
       " 'year': 116}"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.booster().get_score(importance_type='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = len(train)\n",
    "total = pd.concat([train, test], ignore_index=True)\n",
    "temp = pd.get_dummies(total['dow'])\n",
    "temp.columns = [\"weekday\" + str(i) for i in range(7)]\n",
    "total = pd.concat([total, temp], axis = 1)\n",
    "temp = pd.get_dummies(total['month'])\n",
    "temp.columns = [\"month\" + str(i) for i in range(12)]\n",
    "total = pd.concat([total, temp], axis = 1)\n",
    "train, test = total.iloc[:index], total.iloc[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['dow','month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why -1\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_store = pd.read_csv('input/air_store_weather.csv',parse_dates=['calendar_date'])\n",
    "weather_store = weather_store.rename(columns={'calendar_date':'visit_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'].values)\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, weather_store, on = ['air_store_id', 'visit_date'])\n",
    "test = pd.merge(test, weather_store, on = ['air_store_id', 'visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('input/train1.csv', index = False)\n",
    "test.to_csv('input/test1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reserve_features = ['rs1_x', 'rv1_x', 'rs2_x', 'rv2_x',\n",
    "       'rs1_y', 'rv1_y', 'rs2_y', 'rv2_y', 'id', 'total_reserv_sum',\n",
    "       'total_reserv_mean', 'total_reserv_dt_diff_mean']\n",
    "genre_area_features = ['air_genre_name3', 'air_area_name3', 'air_genre_name4',\n",
    "       'air_area_name4', 'air_genre_name5', 'air_area_name5',\n",
    "       'air_genre_name6', 'air_area_name6', 'air_genre_name7',\n",
    "       'air_area_name7', 'air_genre_name8', 'air_area_name8',\n",
    "       'air_genre_name9', 'air_area_name9']\n",
    "#sub_train = train[[col for col in train.columns if col not in reserve_features and col not in genre_area_features]]\n",
    "col = [c for c in train.columns if c not in ['id', 'air_store_id', 'visit_date','visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>air_area_name0</th>\n",
       "      <th>air_area_name1</th>\n",
       "      <th>air_area_name2</th>\n",
       "      <th>air_area_name3</th>\n",
       "      <th>air_area_name4</th>\n",
       "      <th>air_area_name5</th>\n",
       "      <th>air_area_name6</th>\n",
       "      <th>air_area_name7</th>\n",
       "      <th>air_area_name8</th>\n",
       "      <th>...</th>\n",
       "      <th>low_temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>hours_sunlight</th>\n",
       "      <th>solar_radiation</th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>avg_vapor_pressure</th>\n",
       "      <th>avg_local_pressure</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>avg_sea_pressure</th>\n",
       "      <th>cloud_cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>10.86</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.05</td>\n",
       "      <td>12.09</td>\n",
       "      <td>1.966667</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1008.9</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>11.67</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1016.1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.85</td>\n",
       "      <td>12.41</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1015.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1018.8</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>54.3</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.433333</td>\n",
       "      <td>7.1</td>\n",
       "      <td>995.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>998.9</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   air_area_name  air_area_name0  air_area_name1  air_area_name2  \\\n",
       "0           62.0             7.0             6.0            26.0   \n",
       "1           62.0             7.0             6.0            26.0   \n",
       "2           62.0             7.0             6.0            26.0   \n",
       "3           62.0             7.0             6.0            26.0   \n",
       "4           62.0             7.0             6.0            26.0   \n",
       "\n",
       "   air_area_name3  air_area_name4  air_area_name5  air_area_name6  \\\n",
       "0             6.0            78.0             0.0             0.0   \n",
       "1             6.0            78.0             0.0             0.0   \n",
       "2             6.0            78.0             0.0             0.0   \n",
       "3             6.0            78.0             0.0             0.0   \n",
       "4             6.0            78.0             0.0             0.0   \n",
       "\n",
       "   air_area_name7  air_area_name8     ...       low_temperature  \\\n",
       "0             0.0             0.0     ...             -0.766667   \n",
       "1             0.0             0.0     ...              0.100000   \n",
       "2             0.0             0.0     ...              2.533333   \n",
       "3             0.0             0.0     ...              2.266667   \n",
       "4             0.0             0.0     ...              0.333333   \n",
       "\n",
       "   precipitation  hours_sunlight  solar_radiation  avg_wind_speed  \\\n",
       "0            0.0            8.70            10.86        1.700000   \n",
       "1            0.0            9.05            12.09        1.966667   \n",
       "2            0.0            7.50            11.67        3.133333   \n",
       "3            0.0            8.85            12.41        2.400000   \n",
       "4           54.3            1.25             2.40        4.433333   \n",
       "\n",
       "   avg_vapor_pressure  avg_local_pressure  avg_humidity  avg_sea_pressure  \\\n",
       "0                 4.9              1010.1          60.0            1013.1   \n",
       "1                 4.6              1008.9          52.0            1011.9   \n",
       "2                 5.7              1013.1          64.0            1016.1   \n",
       "3                 5.1              1015.8          54.0            1018.8   \n",
       "4                 7.1               995.9          95.0             998.9   \n",
       "\n",
       "   cloud_cover  \n",
       "0          2.5  \n",
       "1          0.5  \n",
       "2          6.0  \n",
       "3          2.3  \n",
       "4          7.5  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE XgboostRegressor: 0.468338734751\n"
     ]
    }
   ],
   "source": [
    "model3 = XGBRegressor(learning_rate=0.1, n_estimators=200, subsample=0.8, colsample_bytree=0.8, max_depth =8)\n",
    "model3.fit(train[sub_col], np.log1p(train['visitors'].values))\n",
    "print('RMSE XgboostRegressor:', RMSLE(np.log1p(train['visitors'].values), model3.predict(train[sub_col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 fitted\n",
      "Model2 fitted\n",
      "Model3 fitted\n",
      "RMSE GradientBoostingRegressor:  0.313813030539\n",
      "RMSE KNeighborsRegressor:  0.449829342612\n",
      "RMSE XgboostRegressor: 0.342089287158\n"
     ]
    }
   ],
   "source": [
    "model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, max_depth =10)\n",
    "\n",
    "model2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=5)\n",
    "\n",
    "model3 = XGBRegressor(learning_rate=0.2, n_estimators=200, subsample=0.8, colsample_bytree=0.8, max_depth =10)\n",
    "\n",
    "model1.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model1 fitted\")\n",
    "model2.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model2 fitted\")\n",
    "model3.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model3 fitted\")\n",
    "print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), model1.predict(train[col])))\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), model2.predict(train[col])))\n",
    "print('RMSE XgboostRegressor:', RMSLE(np.log1p(train['visitors'].values), model3.predict(train[col])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn \n",
    "K = 3: 0.614\n",
    "K = 10: 0.539\n",
    "K = 5: 0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "RMSLE:  0.590998219748\n",
      "1\n",
      "RMSLE:  0.516366702972\n",
      "2\n",
      "RMSLE:  0.510421400952\n"
     ]
    }
   ],
   "source": [
    "# Cross validation on three random selection\n",
    "model2 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, max_depth =10)\n",
    "model1 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=5)\n",
    "model3 = XGBRegressor(learning_rate=0.2, n_estimators=200, subsample=0.8, colsample_bytree=0.8, max_depth =10)\n",
    "models = [model1, model2, model3]\n",
    "\n",
    "train_index = np.random.randint(0, len(train), int(len(train)*0.85))\n",
    "val_index = np.setdiff1d(np.arange(0, len(train)), train_index)\n",
    "X_val, y_val = train.iloc[val_index][col], np.log1p(train.iloc[val_index]['visitors'].values)\n",
    "\n",
    "for i in range(3):\n",
    "    model = models[i]\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    X_train, y_train = train.iloc[train_index][col], np.log1p(train.iloc[train_index]['visitors'].values)\n",
    "    model.fit(X_train[col], y_train)\n",
    "    print('RMSLE: ', RMSLE(y_val, model.predict(X_val)))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predKnn, predGb, predXg = models[0].predict(X_val), models[1].predict(X_val), models[2].predict(X_val)\n",
    "hklee = np.log1p(X_val.hklee_feature)\n",
    "ensemble_df = pd.DataFrame({\"knn\":predKnn, \"gb\":predGb, \"xg\":predXg, \"hk\":hklee})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model = XGBRegressor(learning_rate=0.2, n_estimators=100, subsample=0.8, colsample_bytree=0.8, max_depth = 10)\n",
    "en_model.fit(ensemble_df.values, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50310988304320903"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSLE(y_val, en_model.predict(ensemble_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2672598 ,  2.57132926,  3.16675444, ...,  1.98544082,\n",
       "        2.07966724,  2.19722458])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16590172,  2.25917637,  3.11480641, ...,  2.15567371,\n",
       "        2.08111022,  1.41808837])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.39294076,  2.2871356 ,  3.09318137, ...,  2.06250572,\n",
       "        1.88974106,  1.78981686], dtype=float32)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2].predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.40119738,  1.94591015,  3.29583687, ...,  1.09861229,\n",
       "        1.94591015,  1.79175947])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         3.519119\n",
       "4         2.200934\n",
       "9         3.184366\n",
       "11        2.650192\n",
       "12        2.915902\n",
       "13        2.739233\n",
       "16        2.915902\n",
       "19        3.184366\n",
       "20        2.200934\n",
       "21        2.650192\n",
       "22        2.915902\n",
       "24        3.519119\n",
       "26        2.200934\n",
       "27        2.650192\n",
       "28        2.915902\n",
       "31        3.184366\n",
       "32        2.200934\n",
       "33        2.650192\n",
       "43        3.184366\n",
       "47        2.739233\n",
       "50        2.200934\n",
       "52        2.915902\n",
       "56        2.650192\n",
       "59        3.519119\n",
       "60        3.184366\n",
       "62        2.200934\n",
       "63        2.650192\n",
       "64        2.915902\n",
       "67        3.184366\n",
       "69        2.650192\n",
       "            ...   \n",
       "252035    1.928282\n",
       "252037    1.889312\n",
       "252044    1.743142\n",
       "252051    1.791401\n",
       "252052    1.928282\n",
       "252055    1.889312\n",
       "252056    1.743142\n",
       "252059    1.917453\n",
       "252060    1.906838\n",
       "252061    1.889312\n",
       "252062    1.743142\n",
       "252064    1.928282\n",
       "252065    1.917453\n",
       "252069    1.791401\n",
       "252071    1.917453\n",
       "252073    1.889312\n",
       "252074    1.743142\n",
       "252076    1.928282\n",
       "252079    1.889312\n",
       "252083    1.906838\n",
       "252084    1.889312\n",
       "252085    1.743142\n",
       "252086    1.791401\n",
       "252087    1.928282\n",
       "252089    1.906838\n",
       "252092    1.791401\n",
       "252100    1.917453\n",
       "252101    1.906838\n",
       "252103    1.791401\n",
       "252107    1.889312\n",
       "Name: hklee_feature, Length: 107696, dtype: float64"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(X_val.hklee_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-29 15:58:36.000453\n",
      "RMSE GradientBoostingRegressor:  0.344157162162\n",
      "RMSE KNeighborsRegressor:  0.415170186118\n",
      "2018-01-29 16:28:42.332897\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "#Bojan Tunguz / Surprise Me 2!\n",
    "model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, max_depth =10)\n",
    "model2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "#model3 = XGBRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, colsample_bytree=0.8, max_depth =10)\n",
    "\n",
    "model1.fit(train[col], np.log1p(train['visitors'].values))\n",
    "model2.fit(train[col], np.log1p(train['visitors'].values))\n",
    "#model3.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), model1.predict(train[col])))\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), model2.predict(train[col])))\n",
    "#print('RMSE XGBRegressor: ', RMSLE(np.log1p(train['visitors'].values), model3.predict(train[col])))\n",
    "#test['visitors'] = (model1.predict(test[col]) + model2.predict(test[col]) + model3.predict(test[col])) / 3\n",
    "test['visitors'] = model1.predict(test[col])*0.3 + model2.predict(test[col])*0.3 + model3.predict(test[col])*0.4\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "del train; del data;\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3, 0.3, 0.4: 0.496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['visitors'] = model1.predict(test[col])*0.2 + model2.predict(test[col])*0.5 + model3.predict(test[col])*0.3\n",
    "test['visitors'] = model1.predict(test[col])*0 + model2.predict(test[col])*0.5 + model3.predict(test[col])*0.5\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('input/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
